{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jomGrnTqvzY2"},"source":["# Lab 8: Training Deep Recurrent Neural Network - Part 2\n","\n","- นาย นันท์มนัส ตั้งประเสริฐ, 63070501040\n","- นาย สัณหณัฐ พรมจรรย์, 63070501069\n","\n","Name your file to 1040_1069.ipynb\n","\n","## Lab Instruction - Language Modelling and Text Classification\n","\n","In this lab, you will learn to train a deep recurrent neural network using LSTM with the Keras library using the Tensorflow backend. Your task is to implement the natural language modelling and text generation.\n","\n","```\n","alice_in_wonderland.txt\n","```\n","\n","In class will use alice_in_wonderland as a text file. Then, you will train your language model using RNN-LSTM.\n","\n","\n","\n","- Language model (in Thai): http://bit.ly/language_model_1\n","- Tutorial on how to create a language model (in English): https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n","\n","To evaluate the model, the perplexity measurement is used: https://stats.stackexchange.com/questions/10302/what-is-perplexity\n","\n","Last, fine-tune your model. You have to try different hyperparameter or adding more data. Discuss your result.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mwoJBrLKvzY6"},"source":["#### 1. Load your data"]},{"cell_type":"code","metadata":{"id":"bcWCFsnfvzY7"},"source":["# Import require library\n","from keras import *\n","from keras.preprocessing import text\n","from keras.preprocessing import sequence\n","from sklearn.model_selection import train_test_split\n","\n","import _utils as fn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tensorflow import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLrjxZrNvzZA"},"source":["# Load data\n","import csv\n","\n","# Load data\n","file = open(\"/content/alice_in_wonderland.txt\",\"r\",encoding=\"utf8\", errors='ignore')\n","raw_text = file.read()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"StYO4DI-vzZE","colab":{"base_uri":"https://localhost:8080/","height":56},"outputId":"dff8b0fb-933d-4224-be80-6919bdc773b5","executionInfo":{"status":"ok","timestamp":1698049433949,"user_tz":-420,"elapsed":15,"user":{"displayName":"mmon_shn","userId":"13779798128549135115"}}},"source":["raw_text[:200]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'CHAPTER I.\\nDown the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was rea'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"X-9U1szlRb1q"},"source":["chars = sorted(list(set(raw_text)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wuaAvGjnRkE7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b52d77d7-ed86-4cea-8408-6a40d4574eae","executionInfo":{"status":"ok","timestamp":1698049433949,"user_tz":-420,"elapsed":12,"user":{"displayName":"mmon_shn","userId":"13779798128549135115"}}},"source":["print(\"Total characters: \", len(chars))\n","print(\"Total word: \", len(raw_text.split()))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total characters:  82\n","Total word:  29371\n"]}]},{"cell_type":"markdown","metadata":{"id":"qksGjaIJvzZJ"},"source":["#### 2. Data Preprocessing\n","\n","*Note that only story will be used as a dataset, footnote and creddit are not include.*\n","\n","The symbol '\\n' is indicated the end of the line ``<EOS>``, which is for our model to end the sentence here.\n","\n","To create a corpus for your model. The following code is can be used:</br>\n","*Note that other techniques can be used*\n","\n","```python\n","# cut the text in semi-redundant sequences of maxlen characters.\n","for i in range(0, len(text) - maxlen, step):\n","    sentences.append(text[i: i + maxlen])\n","    next_chars.append(text[i + maxlen])\n","```\n","\n","The code loop through the data from first word to the last word. The maxlen define a next n word for a model to predict.\n"]},{"cell_type":"code","metadata":{"id":"Vj-YPtYsvzZK"},"source":["from keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkIyzqV0RyJy","executionInfo":{"status":"ok","timestamp":1698049433950,"user_tz":-420,"elapsed":10,"user":{"displayName":"mmon_shn","userId":"13779798128549135115"}},"outputId":"ed15a223-18c1-4d02-e1cb-8318ca43f277","colab":{"base_uri":"https://localhost:8080/","height":56}},"source":["# Adding end of string symbol use .replace   to replace data_text with  [  \\n\\n', \" <EOS> \" ]\n","raw_text = raw_text.replace('\\n\\n', \" <EOS> \")\n","raw_text[:200]\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'CHAPTER I.\\nDown the Rabbit-Hole <EOS> \\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister wa'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"AKE389h4vzZO"},"source":["# Preprocessing\n","# Create corpus & Vectorization\n","\n","#Preprocessing\n","# Create corpus & Vectorization\n","\n","tokenizer = text.Tokenizer()\n","\n","# basic cleanup\n","corpus = raw_text.lower().split(\"\\n\")\n","\n","# tokenization\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# create input sequences using list of tokens\n","input_sequences = []\n","for line in corpus:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# pad sequences\n","max_sequence_len = max([len(x) for x in input_sequences])\n","\n","# Pre padding\n","input_sequences = np.array(sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# create predictors and label\n","predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n","\n","# One-hot label\n","label = keras.utils.to_categorical(label, num_classes=total_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07zFy2jotl1F","outputId":"a4803289-a845-477a-e437-f240bb51bd15","executionInfo":{"status":"ok","timestamp":1698049434545,"user_tz":-420,"elapsed":10,"user":{"displayName":"mmon_shn","userId":"13779798128549135115"}}},"source":["print('Max sequence len: %s' % max_sequence_len)\n","print('Total word len: %s' % total_words)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sequence len: 112\n","Total word len: 3162\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zpPDrYsNtgX8","outputId":"04c4b509-c6cd-4716-a92a-658db2c149e8","executionInfo":{"status":"ok","timestamp":1698049434545,"user_tz":-420,"elapsed":9,"user":{"displayName":"mmon_shn","userId":"13779798128549135115"}}},"source":["n_gram_sequence[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3160"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"50nrDM_gtje1","outputId":"f9b39ac5-dc41-45ce-efa4-ab8c96bbfa57","executionInfo":{"status":"ok","timestamp":1698049434545,"user_tz":-420,"elapsed":7,"user":{"displayName":"mmon_shn","userId":"13779798128549135115"}}},"source":["print(predictors[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0 330]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ZzUBdMctjgx","outputId":"eda19b31-0439-4dd5-8390-c8dce54db5d2","executionInfo":{"status":"ok","timestamp":1698049434546,"user_tz":-420,"elapsed":6,"user":{"displayName":"mmon_shn","userId":"13779798128549135115"}}},"source":["print(label[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0. 0. ... 0. 0. 0.]\n"]}]},{"cell_type":"markdown","metadata":{"id":"c-ry6hn0vzZc"},"source":["#### 3. Language Model\n","\n","Define RNN model using LSTM and word embedding representation</br>\n","We will used perplexity as a metrics\n","\n","```python\n","def perplexity(y_true, y_pred):\n","    cross_entropy = keras.backend.categorical_crossentropy(y_true, y_pred)\n","    perplexity = keras.backend.pow(2.0, cross_entropy)\n","    return perplexity\n","```\n","\n","To used custom metrics function > https://keras.io/metrics/\n","\n","For a loss function `categorical_crossentropy` is used, any optimzation method can be applied."]},{"cell_type":"code","metadata":{"id":"upTYjfZNvzZc"},"source":["from keras.layers import Embedding\n","from keras.layers import LSTM\n","from keras.layers import Dropout\n","from keras.layers import Dense\n","import keras.backend"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xDOBkmNYvzZf"},"source":["def perplexity(y_true, y_pred):\n","    cross_entropy = keras.backend.categorical_crossentropy(y_true, y_pred)\n","    perplexity = keras.backend.pow(2.0, cross_entropy)\n","    return perplexity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAgBOsbrvzZj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ece77589-8ddc-4911-d5d5-365bec04b835","executionInfo":{"status":"ok","timestamp":1698049438185,"user_tz":-420,"elapsed":3643,"user":{"displayName":"mmon_shn","userId":"13779798128549135115"}}},"source":["\n","# Define your model\n","# Used Word Embedding\n","\n","model = models.Sequential()\n","model.add(layers.Embedding(total_words, 512,input_length=max_sequence_len-1,name='Embedding'))\n","model.add(layers.LSTM(512, kernel_initializer = 'he_normal',\n","                      dropout=0.1,\n","                      return_sequences=True,\n","                     name='LSTM1'))\n","model.add(layers.LSTM(256, kernel_initializer = 'he_normal',\n","                     dropout=0.1,\n","                     name='LSTM2'))\n","model.add(layers.Dense(total_words, activation='softmax',name='Output'))\n","\n","model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=[perplexity])\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," Embedding (Embedding)       (None, 111, 512)          1618944   \n","                                                                 \n"," LSTM1 (LSTM)                (None, 111, 512)          2099200   \n","                                                                 \n"," LSTM2 (LSTM)                (None, 256)               787456    \n","                                                                 \n"," Output (Dense)              (None, 3162)              812634    \n","                                                                 \n","=================================================================\n","Total params: 5318234 (20.29 MB)\n","Trainable params: 5318234 (20.29 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"65pM7pqNvzZm"},"source":["# Define your model\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=[ perplexity])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8SYfwTCvzZr","outputId":"9f0728c5-234e-4d10-940d-4ef8244b3d7c","colab":{"base_uri":"https://localhost:8080/"}},"source":["from keras.callbacks import EarlyStopping\n","early_stop = EarlyStopping(monitor='loss', verbose=1)\n","\n","history = model.fit(predictors, label,batch_size=32, epochs=100, callbacks=[early_stop])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n"," 67/893 [=>............................] - ETA: 2:16 - loss: 6.7678 - perplexity: 321.3356"]}]},{"cell_type":"markdown","metadata":{"id":"-uZwRT6RvzZu"},"source":["#### 4. Evaluate your model"]},{"cell_type":"code","metadata":{"id":"9g4DkVi3vzZv"},"source":["\n","# Create a function to evaluate your model using perplexity measurment (You can try adding other measurements as well)\n","def evaluate_result(features, label, model ):\n","    model.evaluate(features, label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6-pIYam3ACT"},"source":["evaluate_result(predictors, label, model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XCU3FZjsvzZy"},"source":["#### 5. Text generating"]},{"cell_type":"code","metadata":{"id":"jAd1Kdl3vzZz"},"source":["\n","\n","def generate_text(seedtext, next_words, max_sequence_len, model):\n","  for j in range(next_words):\n","    token_list = tokenizer.texts_to_sequences([seedtext])[0]\n","    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","    #predicted = model.predict_classes(token_list, verbose=0)\n","    predict_x=model.predict(token_list)\n","    predicted =np.argmax(predict_x,axis=1)\n","\n","    output_word = \"\"\n","    for word, index in tokenizer.word_index.items():\n","      if index == predicted:\n","        output_word = word\n","        break\n","    seedtext +=\" \" + output_word\n","  return seedtext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yC3AVscmvzZ4"},"source":["# generate your sample text\n","\n","seed_text = input('Enter your start sentence:')\n","#generate_text , Input , Num_next_word,Max_sequence,Model\n","gen_text = generate_text(seed_text,10,max_sequence_len,model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ZblD2rvZFnF"},"source":["gen_text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**✨Answer: เราจะลด  dropout rate ให้กับแต่ละเลเยอร์ของ model โดย dropout จะช่วยให้ model มีความแม่นยำหรือเข้าใจในการ train model มากกว่าเดิม และได้เพิ่มจำนวน epochs ทำ model มีโอกาสเรียนรู้จาก train data มากขึ้น เห็นได้ชัดว่า loss ลดลง และ perplexity ลดลง และใส่ EarlyStopping เพื่อกำหนดจำนวน training epochs ที่เหมาะสมที่สุดโดยอัตโนมัติ โดยจะตรวจ  loss ระหว่างการ train และหยุดการ train เมื่อเริ่มเกิด loss เพิ่มขึ้นหรือคงที่ ซึ่งบ่งชี้ว่า model ไม่มีการปรับปรุงกับ unseen data ดังนั้นจึงหยุดที่ epoch ที่ 39**"],"metadata":{"id":"9I_AiOggkqZl"}},{"cell_type":"code","source":[],"metadata":{"id":"M7aWrqANk6jT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fLz1ABUgvzaI"},"source":["### More on Natural language Processing and Language model\n","1. https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e\n","2. https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b\n","3. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n","\n","**Music generates by RNN**\n","https://soundcloud.com/optometrist-prime/recurrence-music-written-by-a-recurrent-neural-network\n"]},{"cell_type":"code","metadata":{"id":"SscOleVkXxiG"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PkhYcG9vXxjq"},"source":[],"execution_count":null,"outputs":[]}]}